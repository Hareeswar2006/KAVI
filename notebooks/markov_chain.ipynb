{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765a4c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "321149f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Word Count: 28906\n",
      "Average Word Length: 4.46 characters\n",
      "Loading phonetic dictionary from ../data/cmudict.txt...\n",
      "Dictionary loaded successfully. 125633 words found.\n",
      "The rhyme scheme of the sample stanza is: ABCD\n",
      "No simple alliteration found in the line: 'Success is counted sweetest by those who ne'er succeed.'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import string\n",
    "\n",
    "def calculate_stats(poem_text):\n",
    "    cleaned_text=poem_text.replace(\"--\",\" \")\n",
    "    words=cleaned_text.lower().split()\n",
    "    word_count=len(words)\n",
    "    total_chars=sum(len(word) for word in words)\n",
    "    if word_count > 0:\n",
    "        average_word_length = total_chars/ word_count\n",
    "    else:\n",
    "        average_word_length = 0\n",
    "    return {\n",
    "        \"word_count\": word_count,\n",
    "        \"average_word_length\": average_word_length\n",
    "    }\n",
    "\n",
    "try:\n",
    "    with open(\"../data/clean_poems.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "        poem_text = f.read()\n",
    "    stats = calculate_stats(poem_text)\n",
    "    print(f\"Total Word Count: {stats['word_count']}\")\n",
    "    print(f\"Average Word Length: {stats['average_word_length']:.2f} characters\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: The file was not found.\")\n",
    "\n",
    "def load_cmudict(filepath=\"../data/cmudict.txt\"):\n",
    "    print(f\"Loading phonetic dictionary from {filepath}...\")\n",
    "    pronunciations = {}\n",
    "    variant_regex = re.compile(r'\\(\\d+\\)$')\n",
    "    allowed_chars_regex = re.compile(r\"[^A-Z']\") \n",
    "    try:\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                line = line.strip() \n",
    "                if line.startswith(';;;') or not line: \n",
    "                    continue\n",
    "                parts = line.split(maxsplit=1) \n",
    "                if len(parts) < 2: \n",
    "                    continue\n",
    "                word = parts[0].strip()\n",
    "                word = variant_regex.sub('', word)\n",
    "                word = word.upper()\n",
    "                word = allowed_chars_regex.sub('', word) \n",
    "                if not word:\n",
    "                    continue\n",
    "                    \n",
    "                phonemes_str = parts[1].strip() \n",
    "                phonemes = phonemes_str.split() \n",
    "                if word not in pronunciations:\n",
    "                    pronunciations[word] = phonemes\n",
    "                    \n",
    "        if not pronunciations:\n",
    "             print(\"Warning: The dictionary appears empty after loading. Check the file format.\")\n",
    "             return None\n",
    "        print(f\"Dictionary loaded successfully. {len(pronunciations)} words found.\")\n",
    "        return pronunciations\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The dictionary file '{filepath}' was not found.\")\n",
    "        return None\n",
    "    \n",
    "def detect_rhyme_scheme(stanza, pronunciation_dict):\n",
    "    if not pronunciation_dict:\n",
    "        return \"Error: Dictionary not loaded.\"\n",
    "    lines = stanza.strip().split('\\n')\n",
    "    last_word_sounds = []\n",
    "\n",
    "    for line in lines:\n",
    "        words = line.split()\n",
    "        if words:\n",
    "            clean_word = words[-1].upper().strip(string.punctuation)\n",
    "            if clean_word in pronunciation_dict:\n",
    "                phonemes = pronunciation_dict[clean_word]\n",
    "                last_stress_index = -1\n",
    "                for i in range(len(phonemes) - 1, -1, -1):\n",
    "                    if phonemes[i][-1] in ('1', '2'):\n",
    "                        last_stress_index = i\n",
    "                        break\n",
    "                if last_stress_index != -1:\n",
    "                    rhyming_part = tuple(phonemes[last_stress_index:])\n",
    "                    last_word_sounds.append(rhyming_part)\n",
    "                else:\n",
    "                    last_word_sounds.append(tuple(phonemes))\n",
    "            else:\n",
    "                last_word_sounds.append(None)\n",
    "        else:\n",
    "             last_word_sounds.append(None)\n",
    "\n",
    "    rhyme_groups = {}\n",
    "    scheme = []\n",
    "    next_rhyme_label = 'A'\n",
    "\n",
    "    for sounds in last_word_sounds:\n",
    "        if sounds is None:\n",
    "            scheme.append('X')\n",
    "            continue\n",
    "        if sounds in rhyme_groups:\n",
    "            scheme.append(rhyme_groups[sounds])\n",
    "        else:\n",
    "            rhyme_groups[sounds] = next_rhyme_label\n",
    "            scheme.append(next_rhyme_label)\n",
    "            next_rhyme_label = chr(ord(next_rhyme_label) + 1)\n",
    "            \n",
    "    return \"\".join(scheme)\n",
    "\n",
    "cmudict = load_cmudict()\n",
    "\n",
    "#Testing phonetics\n",
    "if cmudict:\n",
    "    try:\n",
    "        sample_stanza=\"\"\"\n",
    "        Because I could not stop for Death,\n",
    "        He kindly stopped for me;\n",
    "        The carriage held but just ourselves\n",
    "        And Immortality.\n",
    "        \"\"\"\n",
    "        rhyme_scheme = detect_rhyme_scheme(sample_stanza, cmudict)\n",
    "        print(f\"The rhyme scheme of the sample stanza is: {rhyme_scheme}\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: 'clean_poems.txt' was not found.\")\n",
    "else:\n",
    "    print(\"Exiting because the phonetic dictionary could not be loaded.\")\n",
    "\n",
    "\n",
    "def find_alliteration(line):\n",
    "    stop_words = set([\n",
    "        'a', 'an', 'the', 'in', 'on', 'at', 'to', 'for', 'of', \n",
    "        'is', 'am', 'are', 'was', 'were', 'be', 'been', 'being',\n",
    "        'and', 'or', 'but', 'if', 'as', 'by', 'with', 'from',\n",
    "        'i', 'you', 'he', 'she', 'it', 'we', 'they', 'me', 'him', \n",
    "        'her', 'us', 'them', 'my', 'your', 'his', 'its', 'our', 'their' \n",
    "    ])\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    cleaned_line = line.lower().translate(translator)\n",
    "    words = cleaned_line.split()\n",
    "    alliterations = []\n",
    "    num_words = len(words)\n",
    "    for i in range(num_words - 1):\n",
    "        current_word = words[i]\n",
    "        if not current_word or current_word in stop_words:\n",
    "            continue\n",
    "        for j in range(i + 1, num_words):\n",
    "            next_word = words[j]\n",
    "            if not next_word:\n",
    "                continue\n",
    "            if next_word in stop_words:\n",
    "                continue\n",
    "            if current_word[0] == next_word[0]:\n",
    "                alliterations.append((current_word, next_word))\n",
    "            break \n",
    "    return alliterations\n",
    "\n",
    "# Testing find_alliterations\n",
    "try:\n",
    "    sample_line = \"Success is counted sweetest by those who ne'er succeed.\"\n",
    "    alliterative_pairs = find_alliteration(sample_line)\n",
    "            \n",
    "    if alliterative_pairs:\n",
    "        print(f\"Found alliteration in the line: '{sample_line}'\")\n",
    "        for pair in alliterative_pairs:\n",
    "            print(f\"  - {pair[0]} / {pair[1]}\")\n",
    "    else:\n",
    "        print(f\"No simple alliteration found in the line: '{sample_line}'\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "        print(\"Error: 'clean_poems.txt' was not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a2339ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_text(filepath=\"../data/clean_poems.txt\"):\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"Error: The file '{filepath}' was not found.\")\n",
    "        return None\n",
    "    try:\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "            text=f.read()\n",
    "            translator=str.maketrans('','',string.punctuation)\n",
    "            cleaned_text=text.lower().translate(translator)\n",
    "            words=cleaned_text.split()\n",
    "            words=[word for word in words if word]\n",
    "            print(f\"Found {len(words)} tokens.\")\n",
    "            return words\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during text preparation: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73ae398b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28897 tokens.\n",
      "['this', 'is', 'my', 'letter', 'to', 'the', 'world', 'that', 'never', 'wrote', 'to', 'me', 'the', 'simple', 'news', 'that', 'nature', 'told', 'with', 'tender']\n"
     ]
    }
   ],
   "source": [
    "tokens=prepare_text()\n",
    "print(tokens[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "618b8d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def build_markov_model(tokens):\n",
    "    model={}\n",
    "    for i in range(len(tokens)-1):\n",
    "        current_word=tokens[i]\n",
    "        next_word=tokens[i+1]\n",
    "        model[current_word] = model.get(current_word, []) + [next_word]\n",
    "    print(\"Number of unique keys: \",len(model))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b012e57a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique keys:  5524\n",
      "Words following 'the':\n",
      "['world', 'simple', 'request', 'authors', 'purple', 'flag', 'definition', 'distant', 'victory', 'worst']\n",
      "\n",
      "Words following 'love':\n",
      "['of', 'we', 'but', 'i', 'a', 'poured', 'i', 'enough', 'alway', 'is']\n"
     ]
    }
   ],
   "source": [
    "markov_model=build_markov_model(tokens)\n",
    "sample_key = 'the'\n",
    "print(f\"Words following '{sample_key}':\")\n",
    "print(markov_model[sample_key][:10]) \n",
    "sample_key_2 = 'love'\n",
    "print(f\"\\nWords following '{sample_key_2}':\")\n",
    "print(markov_model[sample_key_2][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b85c954a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokens, length=50):    \n",
    "    possible_starts = list(model.keys())\n",
    "    if not possible_starts:\n",
    "        print(\"Error: Model has no keys to start from.\")\n",
    "        return None\n",
    "    current_word = random.choice(possible_starts)\n",
    "    generated = [current_word]\n",
    "    for _ in range(length - 1):\n",
    "        if current_word in model:\n",
    "            possible_next_words = model[current_word]\n",
    "            next_word = random.choice(possible_next_words)\n",
    "            generated.append(next_word)\n",
    "            current_word = next_word\n",
    "        else:\n",
    "            break\n",
    "    return \" \".join(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7fb10bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rest their coming and deem to tie to the chosen child oftener through the summers morn a door i strove to that bore the candle or arctic creature nodding in a hundred years sundered tune without a simple news is that april candid in dishonored grass no means imagined by one consummate bill an amethyst nights wild now that held low to perceive new england town so far the spot and im wife ive got my abode empowered with strict economy subsists till his it is endless be a listener admonished by and rowed him to earths to surmise and\n"
     ]
    }
   ],
   "source": [
    "generated_poem_text = generate_text(markov_model, tokens, length=100) \n",
    "print(generated_poem_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07210558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Artisan PoetBot Initialized ---\n",
      "Type 'analyze', 'generate', or 'quit'.\n",
      "\n",
      "Please paste the poem stanza you want to analyze.\n",
      "(Enter an empty line when you are finished):\n",
      "\n",
      "--- Analysis Results ---\n",
      "Detected Rhyme Scheme: AABCCDDBCC\n",
      "Alliteration Found:\n",
      "  Line 5:\n",
      "    - rock / rock\n",
      "  Line 7:\n",
      "    - care / care\n",
      "  Line 10:\n",
      "    - rock / rock\n",
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: The Main Chatbot Loop\n",
    "\n",
    "# --- Make sure functions from Phase 2 are available ---\n",
    "# (We need detect_rhyme_scheme and find_alliteration)\n",
    "# (If analyzer.py is in the same directory, you could import them, \n",
    "# but for simplicity in a notebook, let's redefine them briefly here \n",
    "# or ensure they were defined in previous cells if you combined files)\n",
    "\n",
    "# --- Assuming these variables exist from previous cells ---\n",
    "# cmudict = load_cmudict() # From Phase 2\n",
    "# tokens = prepare_text_for_markov() # From Phase 3, Step 1\n",
    "# markov_model = build_markov_model(tokens) # From Phase 3, Step 2\n",
    "# generate_text = ... # Function from Phase 3, Step 3\n",
    "# detect_rhyme_scheme = ... # Function from Phase 2, Step 2 (Revised)\n",
    "# find_alliteration = ... # Function from Phase 2, Step 3 (Advanced)\n",
    "print(\"\\n--- Artisan PoetBot Initialized ---\")\n",
    "print(\"Type 'analyze', 'generate', or 'quit'.\")\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        user_choice = input(\"\\nWhat would you like to do? (analyze/generate/quit): \").lower().strip()\n",
    "\n",
    "        if user_choice == 'quit':\n",
    "            print(\"Goodbye!\")\n",
    "            break # Exit the loop\n",
    "\n",
    "        elif user_choice == 'analyze':\n",
    "            print(\"\\nPlease paste the poem stanza you want to analyze.\")\n",
    "            print(\"(Enter an empty line when you are finished):\")\n",
    "            \n",
    "            stanza_lines = []\n",
    "            while True:\n",
    "                line = input()\n",
    "                if line == \"\":\n",
    "                    break\n",
    "                stanza_lines.append(line)\n",
    "            \n",
    "            input_stanza = \"\\n\".join(stanza_lines)\n",
    "            \n",
    "            if not input_stanza.strip():\n",
    "                print(\"No stanza provided.\")\n",
    "                continue\n",
    "\n",
    "            print(\"\\n--- Analysis Results ---\")\n",
    "            # Call Rhyme Scheme Detector\n",
    "            if cmudict:\n",
    "                scheme = detect_rhyme_scheme(input_stanza, cmudict)\n",
    "                print(f\"Detected Rhyme Scheme: {scheme}\")\n",
    "            else:\n",
    "                 print(\"Cannot detect rhyme scheme (dictionary not loaded).\")\n",
    "\n",
    "            # Call Alliteration Finder (line by line)\n",
    "            print(\"Alliteration Found:\")\n",
    "            found_any_alliteration = False\n",
    "            for line_num, line_text in enumerate(input_stanza.split('\\n')):\n",
    "                 pairs = find_alliteration(line_text)\n",
    "                 if pairs:\n",
    "                     found_any_alliteration = True\n",
    "                     print(f\"  Line {line_num + 1}:\")\n",
    "                     for pair in pairs:\n",
    "                         print(f\"    - {pair[0]} / {pair[1]}\")\n",
    "            if not found_any_alliteration:\n",
    "                 print(\"  None detected with the current rule.\")\n",
    "\n",
    "\n",
    "        elif user_choice == 'generate':\n",
    "            print(\"\\n--- Generating Poem Snippet (Markov Chain) ---\")\n",
    "            if markov_model and tokens:\n",
    "                # Generate 50 words this time\n",
    "                generated_output = generate_text(markov_model, tokens, length=50) \n",
    "                if generated_output:\n",
    "                    print(generated_output)\n",
    "                else:\n",
    "                    print(\"Could not generate text.\")\n",
    "            else:\n",
    "                print(\"Cannot generate text (model not ready).\")\n",
    "\n",
    "        else:\n",
    "            print(\"Invalid choice. Please type 'analyze', 'generate', or 'quit'.\")\n",
    "\n",
    "    except EOFError: # Handles pressing Ctrl+D or similar to end input\n",
    "         print(\"\\nInput ended unexpectedly. Goodbye!\")\n",
    "         break\n",
    "    except KeyboardInterrupt: # Handles pressing Ctrl+C\n",
    "         print(\"\\nInterrupted by user. Goodbye!\")\n",
    "         break\n",
    "    except Exception as e:\n",
    "         print(f\"\\nAn unexpected error occurred: {e}\")\n",
    "         # Optionally, you might want to break or add more error handling\n",
    "         # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7477d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kavi_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
